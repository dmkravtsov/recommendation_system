{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекоммендационный алгоритм для заказа рекламы у блоггеров предприятиями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "План: реализовать рекоммендационный алгоритм, основываясь на сравнении тематики текстов авторов и сферы деятельности предприятий.\n",
    "Выводы будут базироваться на сравнении косинусного расстояния попарно между векторами, описывающими индустрию и тематику текстов авторов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package genesis is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import of necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('genesis')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop_words = stopwords.words('english')\n",
    "more_stopwords = ['u', 'im', 'c']\n",
    "stop_words = stop_words + more_stopwords\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "# !pip install pyspellchecker\n",
    "# from spellchecker import SpellChecker\n",
    "# spell = SpellChecker()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "sns.set_theme(context='notebook', style='whitegrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful modules\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('`\"\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = ' '.join(word for word in text.split(',') if word not in stop_words)\n",
    "#     text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n",
    "    text = ' '.join(wordnet_lemmatizer.lemmatize(word)  for word in text.split(','))\n",
    "#     text = correct_spellings(text)\n",
    "    return text\n",
    "\n",
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df.text_clean):\n",
    "        words=[word for word in word_tokenize(tweet)]\n",
    "        corpus.append(words)\n",
    "    return corpus\n",
    "\n",
    "def get_recommendation(top, df, scores):\n",
    "  recommendation = pd.DataFrame(columns = ['Author', 'Name', 'score'])\n",
    "  count = 0\n",
    "  for i in top:\n",
    "      recommendation.at[count, 'Author'] = 7\n",
    "      recommendation.at[count, 'Name'] = df['Name'][i]\n",
    "      recommendation.at[count, 'score'] =  scores[count]\n",
    "      count += 1\n",
    "  return recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author ID</th>\n",
       "      <th>corp_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10811</th>\n",
       "      <td>83</td>\n",
       "      <td>google will show you where to vote it looks li...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Author ID                                         corp_clean\n",
       "10811         83  google will show you where to vote it looks li..."
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load, clean, rename, groupby medians for each autor for posts file\n",
    "df1 = pd.read_csv('posts.csv') \n",
    "df1.rename(columns = {\"Blogger's ID\":'Author ID',\"Number of retrieved inlinks\":'links'}, inplace = True)\n",
    "df1.rename(columns = {\"Number of retrieved comments\":'comments',\"Number of comments\":'commentsNum'}, inplace = True)\n",
    "\n",
    "# split title & text\n",
    "df1['corp'] = df1.Title.fillna('') + \" \" + df1.Content.fillna('')\n",
    "df1 = df1.drop(['Title','Content'], axis=1)\n",
    "# clean text\n",
    "# df1 = df1.set_index('Author ID')\n",
    "df1['corp_clean'] = df1['corp'].apply(clean_text)\n",
    "df1 = df1[['Author ID','corp_clean']]\n",
    "df1.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках данного проекта выбираю автора с ID 7 и его публикацию под ID 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author ID</th>\n",
       "      <th>corp_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>7</td>\n",
       "      <td>opera mini sees  million mobile users in febru...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Author ID                                         corp_clean\n",
       "123          7  opera mini sees  million mobile users in febru..."
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df1.iloc[[123]]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полный текст публикации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'opera mini sees  million mobile users in february up  percent browser maker opera software has released its latest  of the mobile  report this morning which is based on the usage of its opera mini browser for mobile phones each month the conclusion is always the same mobile web usage around the world keeps on growing and growing in february opera mini had over  million users a  increase from january  and more than  increase compared to february  says that the  million plus users viewed more than  billion pages in february which actually represents a  decrease from january opera claims this is because february only has  days compared to january’s  since february  page views have increased by  in february opera mini users generated over  million mb of data with consumption down by  since february  data traffic is up over  the top  countries for opera mini usage in february remained the same with users  mainly centralized in russia indonesia india china ukraine south africa nigeria the united states vietnam and the united kingdom opera claims that people browsing with opera mini in the top  countries according to unique users spent approximately  million in data costs which if tallied for  months represents a potential of  billion per year opera calculates this based on usage and the average cost of browsing in each country opera calculates this number based upon using  per megabyte as a global average while  billion seems like a big reach for opera you can imagine what the users of more popular browsers like mobile safari or android are spending per users opera mini users spent approximately  on average in the month of february which comes out to  per year the heaviest spending occurs in the united states  useryear and the united kingdom  whereas the least spending occurs in india  and south africa  which is mostly due to data costs in these countries as opposed to the amount of data transferred\\tspecifically in north america  february  to february  page views in the top  countries of north america and the caribbean increased by  unique users increased by  and data transferred increased by  in terms of specific sites used in the us among mobile web users google is at the top of the rankings with facebook coming in second and yahoo in third in the united states and canada blackberry handsets occupy the number  spot of course  blackberry takes the top spot because iphone and android users both have powerful browsers nullifying the need to use opera minihowever opera just submitted an application for an opera mini app for the iphone which it claims is up to  times faster than the native browser thanks to its compression and serverside rendering technology  see if the app is approved  crunchbase information'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.corp_clean.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизирую текст автора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing tfidf vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "tfidf_autor = tfidf_vectorizer.fit_transform((sample['corp_clean'])) #fitting and transforming the vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаю сет компаний, выбираю описательный признак деятельности компании для дальнейшей работы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>not a clue adventures</td>\n",
       "      <td>leisure travel  tourism</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name                 Industry\n",
       "735  not a clue adventures  leisure travel  tourism"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('companies.csv')\n",
    "# df = df.set_index('Name')\n",
    "df = df[['Name','Industry']]\n",
    "df['Industry'] = df['Industry'].apply(clean_text)\n",
    "df.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Векторизирую описательную часть сферы деятельности компаний, составляю матрицу косинусных расстояний между векторами автора и компаний:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "tfidf_company = tfidf_vectorizer.transform(df['Industry'])\n",
    "cos_similarity_tfidf = map(lambda x: cosine_similarity(tfidf_company, x),tfidf_autor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = list(cos_similarity_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге получаю отсортированный фрейм данных с показателем косинусного расстояния (score) для данного автора по представленным компаниям.\n",
    "Чем выше этот показатель - тем ближе по тематике публикация автора к сфере деятельности компании. \n",
    "\n",
    "Опытным путем в дальнейшем следует установить минимальную границу данного показателя, по которой будет происходть отбор кандидатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>7</td>\n",
       "      <td>jwe corporation</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>7</td>\n",
       "      <td>xceed systems</td>\n",
       "      <td>0.122426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>7</td>\n",
       "      <td>delhi recuiters pvt. ltd.</td>\n",
       "      <td>0.181756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>7</td>\n",
       "      <td>medvensys, llc</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>7</td>\n",
       "      <td>vitalia salud</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Author                       Name     score\n",
       "678      7            jwe corporation       0.0\n",
       "123      7              xceed systems  0.122426\n",
       "63       7  delhi recuiters pvt. ltd.  0.181756\n",
       "351      7             medvensys, llc       0.0\n",
       "343      7              vitalia salud       0.0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = sorted(range(len(output[0])), key=lambda i: output[0][i], reverse=True)\n",
    "list_scores = [output[0][i][0] for i in top]\n",
    "get_recommendation(top,df, list_scores).sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: применив данный подход к группе блоггеров из класса 2 (шаг кластеризации блоггеров), можно отобрать те компании, кому можно их рекомендовать для организации рекламных публикаций."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "004de6046f1b3d314f33fdb43a2dc798b2646e5600efd8df5066c8b63a00ff6d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
